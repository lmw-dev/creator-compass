#!/usr/bin/env python3

"""
æ‰¹é‡åˆ†ææ‰€æœ‰åšä¸»è§†é¢‘ç›®å½•çš„è„šæœ¬
"""

import sys
import os
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.ai_outreach.blogger_analyzer import BloggerAnalyzer
from src.ai_outreach.generator import ScriptGenerator
from src.ai_outreach.utils.logger import logger, setup_logger
from src.ai_outreach.utils.exceptions import AIOutreachException

class BatchBloggerAnalyzer:
    """æ‰¹é‡åšä¸»åˆ†æå™¨"""
    
    def __init__(self):
        self.blogger_analyzer = BloggerAnalyzer()
        self.script_generator = ScriptGenerator()
        self.results = []
        self.failed_analyses = []
        
    def find_blogger_directories(self, base_path: Path) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰åšä¸»ç›®å½•"""
        blogger_dirs = []
        
        if not base_path.exists():
            logger.error(f"åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            return blogger_dirs
            
        for item in base_path.iterdir():
            if item.is_dir() and self.is_blogger_directory(item):
                blogger_dirs.append(item)
                
        # æŒ‰ç›®å½•åæ’åº
        blogger_dirs.sort(key=lambda x: x.name)
        logger.info(f"æ‰¾åˆ° {len(blogger_dirs)} ä¸ªåšä¸»ç›®å½•")
        
        return blogger_dirs
    
    def is_blogger_directory(self, directory: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åšä¸»ç›®å½•"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åšä¸»ä¿¡æ¯æ–‡ä»¶
        info_files = list(directory.glob("äººç‰© - *.md"))
        if not info_files:
            return False
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘æ–‡ä»¶
        video_extensions = ['.mp4', '.avi', '.mkv', '.mov']
        for ext in video_extensions:
            if list(directory.glob(f"*{ext}")):
                return True
                
        return False
    
    def analyze_single_blogger(self, blogger_dir: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªåšä¸»ç›®å½•"""
        result = {
            'directory': str(blogger_dir),
            'blogger_name': '',
            'status': 'pending',
            'error': None,
            'report_path': None,
            'start_time': time.time(),
            'end_time': None,
            'duration': 0
        }
        
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æåšä¸»ç›®å½•: {blogger_dir.name}")
            
            # åˆ†æåšä¸»
            analysis_result = self.blogger_analyzer.analyze_blogger_folder(blogger_dir)
            
            # æå–åšä¸»åç§°
            blogger_name = analysis_result['blogger_info'].name
            result['blogger_name'] = blogger_name
            
            # ç”ŸæˆæŠ¥å‘Š
            report_path = self.script_generator.generate_blogger_comprehensive_report(analysis_result)
            result['report_path'] = str(report_path)
            result['status'] = 'success'
            
            logger.info(f"âœ… åšä¸»åˆ†æå®Œæˆ: {blogger_name} -> {report_path.name}")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"âŒ åšä¸»åˆ†æå¤±è´¥: {blogger_dir.name} - {e}")
            
        finally:
            result['end_time'] = time.time()
            result['duration'] = result['end_time'] - result['start_time']
            
        return result
    
    def batch_analyze(self, base_path: str, skip_existing: bool = True, 
                     delay_between_analyses: int = 5) -> Dict[str, Any]:
        """æ‰¹é‡åˆ†ææ‰€æœ‰åšä¸»"""
        base_path = Path(base_path)
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åšä¸»åˆ†æ: {base_path}")
        
        # æŸ¥æ‰¾æ‰€æœ‰åšä¸»ç›®å½•
        blogger_dirs = self.find_blogger_directories(base_path)
        
        if not blogger_dirs:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•åšä¸»ç›®å½•")
            return {'total': 0, 'success': 0, 'failed': 0, 'results': []}
        
        total_dirs = len(blogger_dirs)
        success_count = 0
        failed_count = 0
        
        logger.info(f"ğŸ“‹ è®¡åˆ’åˆ†æ {total_dirs} ä¸ªåšä¸»ç›®å½•")
        
        # é€ä¸ªåˆ†æ
        for i, blogger_dir in enumerate(blogger_dirs, 1):
            logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_dirs} ({i/total_dirs*100:.1f}%)")
            
            # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æŠ¥å‘Š
            if skip_existing and self.has_existing_report(blogger_dir):
                logger.info(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨æŠ¥å‘Šçš„åšä¸»: {blogger_dir.name}")
                continue
            
            # åˆ†æåšä¸»
            result = self.analyze_single_blogger(blogger_dir)
            self.results.append(result)
            
            if result['status'] == 'success':
                success_count += 1
            else:
                failed_count += 1
                self.failed_analyses.append(result)
            
            # å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            if i < total_dirs and delay_between_analyses > 0:
                logger.info(f"â±ï¸  ç­‰å¾… {delay_between_analyses} ç§’...")
                time.sleep(delay_between_analyses)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š  
        summary = {
            'total': total_dirs,
            'processed': len(self.results),
            'success': success_count,
            'failed': failed_count,
            'skipped': total_dirs - len(self.results),
            'results': self.results,
            'failed_analyses': self.failed_analyses
        }
        
        self.print_summary(summary)
        return summary
    
    def has_existing_report(self, blogger_dir: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åˆ†ææŠ¥å‘Š"""
        try:
            # æå–åšä¸»åç§°
            info_files = list(blogger_dir.glob("äººç‰© - *.md"))
            if not info_files:
                return False
                
            filename = info_files[0].stem
            name_match = re.search(r'äººç‰©\\s*-\\s*(.+)', filename)
            blogger_name = name_match.group(1).strip() if name_match else "Unknown"
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²æœ‰æŠ¥å‘Š
            output_dir = Path("outputs")
            existing_reports = list(output_dir.glob(f"åšä¸»ç»¼åˆåˆ†æ-{blogger_name}-*.md"))
            
            return len(existing_reports) > 0
            
        except Exception:
            return False
    
    def print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ‰¹é‡åˆ†æå®Œæˆç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"æ€»ç›®å½•æ•°: {summary['total']}")
        logger.info(f"å·²å¤„ç†: {summary['processed']}")
        logger.info(f"æˆåŠŸ: {summary['success']}")
        logger.info(f"å¤±è´¥: {summary['failed']}")
        logger.info(f"è·³è¿‡: {summary['skipped']}")
        
        if summary['failed_analyses']:
            logger.info("âŒ å¤±è´¥çš„åˆ†æ:")
            for failed in summary['failed_analyses']:
                logger.info(f"  - {Path(failed['directory']).name}: {failed['error']}")
        
        if summary['success'] > 0:
            logger.info("âœ… æˆåŠŸç”Ÿæˆçš„æŠ¥å‘Š:")
            for result in summary['results']:
                if result['status'] == 'success':
                    logger.info(f"  - {result['blogger_name']}: {Path(result['report_path']).name}")

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logger()
    
    # é…ç½®å‚æ•°
    BASE_PATH = "/Users/liumingwei/ä¸ªäººæ–‡æ¡£åŒæ­¥/05-å·¥ä½œèµ„æ–™/01-åšä¸»è§†é¢‘"
    SKIP_EXISTING = True  # æ˜¯å¦è·³è¿‡å·²æœ‰æŠ¥å‘Šçš„åšä¸»
    DELAY_SECONDS = 10   # æ¯æ¬¡åˆ†æé—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
    
    try:
        analyzer = BatchBloggerAnalyzer()
        
        logger.info("ğŸ¯ AIå¤–è”å†›å¸ˆ - æ‰¹é‡åšä¸»åˆ†æå·¥å…·")
        logger.info(f"ğŸ“ åˆ†æç›®å½•: {BASE_PATH}")
        logger.info(f"â­ï¸  è·³è¿‡å·²æœ‰æŠ¥å‘Š: {'æ˜¯' if SKIP_EXISTING else 'å¦'}")
        logger.info(f"â±ï¸  åˆ†æé—´éš”: {DELAY_SECONDS}ç§’")
        
        # å¼€å§‹æ‰¹é‡åˆ†æ
        summary = analyzer.batch_analyze(
            base_path=BASE_PATH,
            skip_existing=SKIP_EXISTING,
            delay_between_analyses=DELAY_SECONDS
        )
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        import json
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = f"outputs/æ‰¹é‡åˆ†æç»“æœ_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        logger.error(f"ğŸ’¥ æ‰¹é‡åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    import re
    main()