#!/usr/bin/env python3

"""
é«˜çº§æ‰¹é‡åˆ†æè„šæœ¬ - å¸¦æœ‰è¯¦ç»†é…ç½®é€‰é¡¹
"""

import sys
import argparse
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡åˆ†æåšä¸»è§†é¢‘ç›®å½•')
    parser.add_argument('--base-path', 
                       default='/Users/liumingwei/ä¸ªäººæ–‡æ¡£åŒæ­¥/05-å·¥ä½œèµ„æ–™/01-åšä¸»è§†é¢‘',
                       help='åšä¸»è§†é¢‘åŸºç¡€ç›®å½•è·¯å¾„')
    parser.add_argument('--skip-existing', action='store_true', default=True,
                       help='è·³è¿‡å·²æœ‰åˆ†ææŠ¥å‘Šçš„åšä¸»')
    parser.add_argument('--delay', type=int, default=10,
                       help='æ¯æ¬¡åˆ†æé—´çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--start-from', type=int, default=1,
                       help='ä»ç¬¬å‡ ä¸ªåšä¸»ç›®å½•å¼€å§‹åˆ†æ')
    parser.add_argument('--max-count', type=int, default=0,
                       help='æœ€å¤šåˆ†æå¤šå°‘ä¸ªåšä¸»ï¼ˆ0è¡¨ç¤ºå…¨éƒ¨ï¼‰')
    parser.add_argument('--dry-run', action='store_true',
                       help='ä»…åˆ—å‡ºè¦åˆ†æçš„ç›®å½•ï¼Œä¸æ‰§è¡Œå®é™…åˆ†æ')
    parser.add_argument('--include-pattern', type=str,
                       help='åªåŒ…å«åŒ¹é…æ­¤æ¨¡å¼çš„ç›®å½•å')
    parser.add_argument('--exclude-pattern', type=str,
                       help='æ’é™¤åŒ¹é…æ­¤æ¨¡å¼çš„ç›®å½•å')
    
    args = parser.parse_args()
    
    # å¯¼å…¥æ‰¹é‡åˆ†æå™¨
    from batch_analyze_bloggers import BatchBloggerAnalyzer
    from src.ai_outreach.utils.logger import setup_logger
    
    setup_logger()
    
    base_path = Path(args.base_path)
    analyzer = BatchBloggerAnalyzer()
    
    print("ğŸ¯ AIå¤–è”å†›å¸ˆ - é«˜çº§æ‰¹é‡åˆ†æå·¥å…·")
    print("=" * 50)
    print(f"ğŸ“ åŸºç¡€ç›®å½•: {base_path}")
    print(f"â­ï¸  è·³è¿‡å·²æœ‰: {'æ˜¯' if args.skip_existing else 'å¦'}")
    print(f"â±ï¸  åˆ†æé—´éš”: {args.delay}ç§’")
    print(f"ğŸ å¼€å§‹ä½ç½®: ç¬¬{args.start_from}ä¸ª")
    if args.max_count > 0:
        print(f"ğŸ“Š æœ€å¤§æ•°é‡: {args.max_count}ä¸ª")
    if args.include_pattern:
        print(f"âœ… åŒ…å«æ¨¡å¼: {args.include_pattern}")
    if args.exclude_pattern:
        print(f"âŒ æ’é™¤æ¨¡å¼: {args.exclude_pattern}")
    print("=" * 50)
    
    # æŸ¥æ‰¾æ‰€æœ‰åšä¸»ç›®å½•
    all_dirs = analyzer.find_blogger_directories(base_path)
    
    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
    filtered_dirs = []
    for blogger_dir in all_dirs:
        # åº”ç”¨åŒ…å«/æ’é™¤æ¨¡å¼
        if args.include_pattern and args.include_pattern not in blogger_dir.name:
            continue
        if args.exclude_pattern and args.exclude_pattern in blogger_dir.name:
            continue
        filtered_dirs.append(blogger_dir)
    
    # åº”ç”¨å¼€å§‹ä½ç½®å’Œæœ€å¤§æ•°é‡é™åˆ¶
    start_idx = max(0, args.start_from - 1)
    if args.max_count > 0:
        end_idx = min(len(filtered_dirs), start_idx + args.max_count)
        target_dirs = filtered_dirs[start_idx:end_idx]
    else:
        target_dirs = filtered_dirs[start_idx:]
    
    print(f"ğŸ“‹ æ€»ç›®å½•æ•°: {len(all_dirs)}")
    print(f"ğŸ” è¿‡æ»¤å: {len(filtered_dirs)}")
    print(f"ğŸ¯ å°†åˆ†æ: {len(target_dirs)}")
    
    if args.dry_run:
        print("\\nğŸ” é¢„è§ˆæ¨¡å¼ - å°†è¦åˆ†æçš„ç›®å½•:")
        for i, blogger_dir in enumerate(target_dirs, start_idx + 1):
            print(f"  {i:2d}. {blogger_dir.name}")
        return
    
    if not target_dirs:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›®å½•")
        return
    
    # ç¡®è®¤å¼€å§‹
    response = input(f"\\nâ“ ç¡®å®šè¦åˆ†æ {len(target_dirs)} ä¸ªåšä¸»ç›®å½•å—ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        print("â¹ï¸  åˆ†æå·²å–æ¶ˆ")
        return
    
    # å¼€å§‹æ‰¹é‡åˆ†æ
    print("\\nğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ...")
    
    # ä¿®æ”¹analyzerä»¥æ”¯æŒæŒ‡å®šç›®å½•åˆ—è¡¨
    analyzer.results = []
    analyzer.failed_analyses = []
    
    success_count = 0
    failed_count = 0
    
    for i, blogger_dir in enumerate(target_dirs, 1):
        print(f"\\nğŸ“Š [{i}/{len(target_dirs)}] {blogger_dir.name}")
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡
        if args.skip_existing and analyzer.has_existing_report(blogger_dir):
            print("â­ï¸  è·³è¿‡ï¼ˆå·²æœ‰æŠ¥å‘Šï¼‰")
            continue
        
        # åˆ†æåšä¸»
        result = analyzer.analyze_single_blogger(blogger_dir)
        analyzer.results.append(result)
        
        if result['status'] == 'success':
            success_count += 1
            print(f"âœ… å®Œæˆ ({result['duration']:.1f}ç§’)")
        else:
            failed_count += 1
            analyzer.failed_analyses.append(result)
            print(f"âŒ å¤±è´¥: {result['error']}")
        
        # å»¶è¿Ÿ
        if i < len(target_dirs) and args.delay > 0:
            print(f"â±ï¸  ç­‰å¾… {args.delay} ç§’...")
            import time
            time.sleep(args.delay)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    summary = {
        'total_found': len(all_dirs),
        'filtered': len(filtered_dirs),
        'processed': len(analyzer.results),
        'success': success_count,
        'failed': failed_count,
        'config': vars(args),
        'results': analyzer.results,
        'failed_analyses': analyzer.failed_analyses,
        'timestamp': datetime.now().isoformat()
    }
    
    analyzer.print_summary(summary)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f"outputs/é«˜çº§æ‰¹é‡åˆ†æ_{timestamp}.json"
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

if __name__ == "__main__":
    main()